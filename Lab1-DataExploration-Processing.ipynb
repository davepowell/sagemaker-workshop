{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Data Exploration & Processing\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "   In this lab, we'll explore the pre-processed data to understand the data, determine which features may be impacting to the ML question we are trying to ask so we can ultimately create training/validation/test datasets that will be used for training and evaluation.  \n",
    "   \n",
    "   This process is typically iterative in model development so in a real-world scenario you would typically try multiple experiments until you are able to best meet your objective metric.  \n",
    "      \n",
    "   * **ML Question**: Is transaction *n* a recurring payment such as a subscription or membership?\n",
    "   * **Lab Outcome:**: The outcome of this lab is to create a training dataset that we will then use in Lab2\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Information:**\n",
    "   \n",
    "   1. This workshop is based on the open public dataset below containing purchase card information from State of Oklahoma: \n",
    "     * https://catalog.data.gov/dataset?groups=local&tags=pcard&organization=state-of-oklahoma\n",
    "     * Dataset: Purchase Card (PCard) Fiscal Year 2015\n",
    "     * Description:  \"*This dataset contains information on purchases made through the purchase card programs administered by the state and higher ed institutions. The purchase card information will be updated monthly after the end of the month.*\" \n",
    "     \n",
    "     \n",
    "   2. The dataset above has been pre-processed to:\n",
    "   \n",
    "    * Include MCC Labels (numeric IDs) mapping to the Merchant Category Code (MCC) in the original dataset.  The MCC Labels were pulled using the following public repository: https://github.com/greggles/mcc-codes\n",
    "    \n",
    "\n",
    "**Features on Input**: \n",
    "   \n",
    " Base Features (Part of original dataset):\n",
    "   * **Year-Month:** yyyymm (Example: 201407)\n",
    "   * **Agency Num:** Unique Agency Number (Payee) \n",
    "   * **Agency Name:** Name of agency (Payee)\n",
    "   * **Cardholder Last Name:** Last Name of Individual Card Holder\n",
    "   * **Cardholder First Name:** First Initial of First Name of Individual Card Holder\n",
    "   * **Description:** Text - Purchase Description (Variable)\n",
    "   * **Amount:** Amount of purchase\n",
    "   * **Vendor:** Payee Name\n",
    "   * **Transaction Date:** Date of purchase in *mm/dd/yy hh:ss* format\n",
    "   * **Posted Date:** Date transaction posted in *mm/dd/yy hh:ss* format\n",
    "   * **Merchant Category Code (MCC):** Description of Merchant Category Code\n",
    "   \n",
    "Additional Features Added: \n",
    " \n",
    "   * **MCC_ID:** Numeric representation of Merchant Category Code (MCC)\n",
    "   * **Recurring_Label:** Label indicating a recurring payment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#data that was cloned from the github repository ~ loaded into storage on this notebook under /data\n",
    "input_data = './data/PCARD-workshop-raw.csv'\n",
    "\n",
    "#read data into a pandas dataframe\n",
    "df = pd.read_csv(input_data)\n",
    "\n",
    "#view the first n rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull in the libraries we will use for this lab ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "import seaborn as sns                             # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "import os                                         # For manipulating filepath names\n",
    "import sagemaker                                  # Amazon SageMaker's Python SDK provides many helper functions\n",
    "from sagemaker.predictor import csv_serializer    # Converts strings for HTTP POST requests on inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Explore Data & Feature Processing\n",
    "\n",
    "In this step, we'll explore the data to understand and shape the data that we'll utilize for training in the next lab. \n",
    "\n",
    "### Labels\n",
    "\n",
    "In the lab, we will be utilizing [Amazon SageMaker: XGBoost Built-In Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html).   XGBoost is a supervised learning algorithm so our data must be labeled.  The data has been pre-labeled using the following indicators of a recurring payment for labeling:\n",
    "     \n",
    "   1) One of the following, in combination with #2: \n",
    "   \n",
    "   * **Description**: Contains any of the following text in the description:\n",
    "         -   contains ‘recurr’,'subscript', or 'member' anywhere inside text \n",
    "         -   contains ‘monthly’ inside text \n",
    "     \n",
    "   * **MCC_ID**: MCC Code in alignment with recurring payment\n",
    "         - 5968 : CONTINUITY/SUBSCRIPTION MERCHANTS\n",
    "      \n",
    "     **~AND~**\n",
    "      \n",
    "\n",
    "   2) Monthly payment (per Cardholder ID) of the same amount (+/- $5) for 3 or more months on same day (+/- 5 days) to same Vendor\n",
    "             \n",
    "\n",
    "\n",
    "*Note: The method for labeling above is not prescriptive in how it should be done but providing the base logic for how this dataset was labeled based on a set of assumptions made. Also, our dataset includes only a single year so recurring payments that happen on a yearly basis may perform poorly as they are not labeled in the dataset.*\n",
    "\n",
    "--- \n",
    "\n",
    "#### Label: Recurring_Label\n",
    "\n",
    "Let's view the distribution of our labeled data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Recurring_Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would anticipate, the majority of transactions are not labeled as recurring payments. XGBoost is effective on datasets even when the label is distribution is skewed.  There are methods for handling class imbalance but for now we will continue with our first experiment as a baseline.  We can also see that there are ~211,080 total records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Distribution:**\n",
    "\n",
    "Let's first take a look at how our features are distributed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=df[column], columns='% observations', normalize='columns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Feature Observations** \n",
    "\n",
    "  * **Agency Name:** There are 115 different agencies on input\n",
    "  * **Description:** The description field is highly variable with 46,888 different descriptions. Note: This field was also used as a basis for a portion of our labeling logic\n",
    "  * **Vendor:** Vendor is also highly variable. with 47,877 different descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for each numeric features\n",
    "display(df.describe())\n",
    "%matplotlib inline\n",
    "hist = df.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numeric Feature Observations** \n",
    "\n",
    "  * **Posted_Date_yy & Transaction_Date_yy:** All records have the same value (2014) which would be expected given the dataset captures a specific year. In the current model context, this feature is unlikely to add predictive value. \n",
    "  * **MCC_ID:** Although this feature is numeric, it is an ID that corresponds to a categorical feature called 'Merchant Category Code (MCC)' so the MCC_ID feature is really a numeric representation of Merchant Category Code (MCC) using integer encoding.\n",
    "  \n",
    "  * **Posted_Date_mm & Transaction_Date_mm:** The input data only contains data from July to December. In this case we are trying to predict recurring payments based on historical data. However, if we were trying to predict whether a current transaction was a recurring payment, we would need to assess how well our model could generalize in January. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data and model we are building, let's now drop the features that don't contribute to the variable we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = df.drop(['Posted_Date_yy', 'Transaction_Date_yy'], axis=1)\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Correlation:**\n",
    "    \n",
    "Let's now take a look at how our features relate to our target variable (recurring)  that we are trying to predict..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(model_data.corr(),annot=True,linecolor='white',linewidths=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations**: \n",
    "\n",
    "\n",
    "1. Highly Correlated Features: \n",
    "    - Year-Month & Posted_Date_mm & Transaction_Date_mm:  Because these features have a high correlation, there is little value in having all three features on input.  If we had a multi-year dataset, the year-month or the _yy features that were removed above would potentially have more value.  In this case, we will keep one of the _mm (Transaction_Date_mm) features and drop the others.  \n",
    "    - Posted_Date_dd & Transaction_Date_dd: Because these features have a high correlation and there is not expected to be a impact on defining a recurring subscription in looking at the posted date vs transaction date, we will drop Posted_Date_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_data = model_data.drop(['Posted_Date_mm', 'Year-Month','Posted_Date_dd'], axis=1)\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process categorical feature(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify categorical features\n",
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop categorical features unlikely to contribute too the predictive value**\n",
    "\n",
    "   - **Vendor and Description**:  As we discovered in our data exploration above, vendor and description were highly variable.  We also utilized fuzzy matches during our labeling from the Description where the text could be indicative of a recurring payment. \n",
    "   - **Agency Name**: The agency name is a categorical representation of the agency number so we will drop the unnecessary categorical feature.\n",
    "   - **Merchant Category Code (MCC)**: This is a categorical representation of the MCC_ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.drop(['Vendor', 'Description','Agency Name','Merchant Category Code (MCC)'], axis=1)\n",
    "model_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only categorical feature we have left is our label 'Recurring_Label'.  XGBoost requires a labeled dataset so we must convert our labels from 'Yes' or 'No' to '1' or '0' to put it a  format XGBoost will understand for the objective metric we define.  There are many methods to convert our categorical features to numeric. For our purposes, we are going to use [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) for one-hot encoding. \n",
    "\n",
    "The expectected result:\n",
    "\n",
    "*Original*\n",
    "\n",
    "| Each Transaction | Recurring_Label |    \n",
    "| -------- | -------------- |\n",
    "| 1  |    Yes            |\n",
    "| 2 | No   | \n",
    "| 3 | No |\n",
    "| 4 | Yes | \n",
    "\n",
    "*After one-hot encoding* \n",
    "\n",
    "| Each Transaction | Recurring_Label_No |  Recurring_Label_Yes |   \n",
    "| -------- | -------------- | ------- |\n",
    "| 1  |    0            |   1\n",
    "| 2 | 1   |   0 |\n",
    "| 3 | 1 |  0 |\n",
    "| 4 | 0 | 1 | \n",
    "\n",
    "The conversion above will happen in-place and will not impact our other features (Ex. Cardholder ID, Amount, etc).  We will then be able to use one of the columns created above on training as our label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our label Subscription_Recurring to numeric (one-hot encoding)\n",
    "model_data = pd.get_dummies(model_data)\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 3: Write our training dataset to S3 \n",
    "\n",
    "In this step, we'll:\n",
    "   * Split our labeled dataset up into train, validation, and test datasets \n",
    "   * Upload train and validation datasets to S3 \n",
    "   \n",
    "For simplicity in the lab environment, we are going to upload to the default Amazon S3 bucket used by the SageMaker session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Data**\n",
    "\n",
    "When building a model whose primary goal is to predict a target value on new data, it is important to understand overfitting. Supervised learning models are designed to minimize error between their predictions of the target value and actuals, in the data they are given. This last part is key, as frequently in their quest for greater accuracy, machine learning models bias themselves toward picking up on minor idiosyncrasies within the data they are shown. These idiosyncrasies then don't repeat themselves in subsequent data, meaning those predictions can actually be made less accurate, at the expense of more accurate predictions in the training phase.\n",
    "\n",
    "The most common way of preventing this is to build models with the concept that a model shouldn't only be judged on its fit to the data it was trained on, but also on \"new\" data. There are several different ways of operationalizing this, holdout validation, cross-validation, leave-one-out validation, etc. For our purposes, we'll simply randomly split the data into 3 uneven groups. The model will be trained on 70% of data, it will then be evaluated on 20% of data to give us an estimate of the accuracy we hope to have on \"new\" data, and 10% will be held back as a final testing dataset which will be used later on.\n",
    "\n",
    "| Train | Validation | Test |\n",
    "| ------ | ------- | ------| \n",
    "| <..............70%.............> |  <....20%....> |  <..10%..>  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])   # Randomly sort the data then split out first 70%, second 20%, and last 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_data['Recurring_Label_Yes'], train_data.drop(['Recurring_Label_No', 'Recurring_Label_Yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([validation_data['Recurring_Label_Yes'], validation_data.drop(['Recurring_Label_No', 'Recurring_Label_Yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Training/Validation Data in S3 \n",
    "\n",
    "The key artifact created from this lab is our training, validation and test datasets that we will use for training and validating our model. XGBoost accepts both a train and optionally a validation dataset on input for training and validation.  We need to ensure our data is loaded to an expected data source for SageMaker training so we will load our data to S3.  \n",
    "\n",
    "Keep in mind, each algorithm has different options for data input channels. This [quick reference](https://github.com/awsdocs/amazon-sagemaker-developer-guide/blob/master/doc_source/sagemaker-algo-docker-registry-paths.md) provides guidance on common parameters for built-in-algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "data_prefix = 'workshop/data'\n",
    "\n",
    "# bucket path to upload our datasets to\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 \n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(data_prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(data_prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data Uploaded to S3 \n",
    "\n",
    "Once the training job is complete, the model will be output to an S3 bucket.  This model artifact will be used for hosting our model and getting predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Markdown\n",
    "\n",
    "s3_data = 'https://s3.console.aws.amazon.com/s3/buckets/'+ bucket + '/' + data_prefix + '/?region=us-east-1&tab=overview'\n",
    "display(Markdown('S3 Data for Training/Evaluation: [link]('+s3_data+')'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 4: Dataprocessing options *(Optional)*\n",
    "\n",
    "In this notebook and during experimentation on smaller datasets, it's common to do minor pre-processing within a notebook.   However, when processing large datasets at scale or creating a operational process that includes data processing for training/inference it's often not practical to continue to perform data processing in a notebook because of the following reasons: \n",
    "   \n",
    "   * **Cost Optimization:** Performing large data processing on the notebook instance directly can require that you utilize a large notebook instance type than needed.  Because notebook instances generally stay up for longer periods of time, you're paying for unnecessary compute/memory during the time you are processing data.   As an example, the simple data preprocessing done in this notebook on ~33MB of data to prepare our training dataset is not possible on a ml.t2.medium instance due to memory errors.  As a result, we utilize a larger notebook size (ml.t3.large) to be able to process data in the notebook instance\n",
    "   \n",
    "   \n",
    "   * **Operational Effectiveness & Scalability:** Deploying a model to production requires the data for inference to be in a specific format based on the training of that model.  It's not scalable to have all of the data preprocessing logic contained in a notebook that can't be integrated with other systems and solutons.   In addition, an operationally efficient retraining strategy should automate as much as possible including formatting the data for training. \n",
    "   \n",
    "   \n",
    "   * **Speed:** Speed up processing time by utilizing larger compute/memory \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Processing \n",
    "\n",
    "AWS offers several options for effective data cleansing and pre-processing for data science workloads.  One of those includes  [Amazon SageMaker Processing](https://aws.amazon.com/blogs/aws/amazon-sagemaker-processing-fully-managed-data-processing-and-model-evaluation/).  SageMaker processing offers fully managed infrastructure for data processing and model evaluation. Amazon SageMaker Processing includes a Python SDK that uses SageMaker's built-in containerr for scikit-learn as well as the capability to use your own Docker images without having  to conform to any Docker image specification.  \n",
    "\n",
    "Using the same dataset we imported in Step 1, let's demonstrate how we can take advantage of secondary compute for processing tasks with SageMaker Processing. Much of the preprocessing was done in advance to simplify the workshop and most of what we did above was drop features as well as convert categorical features to numeric.  \n",
    "\n",
    "Let's read the original dataset into a DataFrame to review original contents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data into a pandas dataframe\n",
    "dfp = pd.read_csv(input_data)\n",
    "\n",
    "#view the first n rows\n",
    "dfp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload raw data to S3 ~ where SageMaker Processing will pick it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path_to_data = sagemaker.Session().upload_data(bucket=bucket, \n",
    "                                                  path='./data/PCARD-workshop-raw.csv', \n",
    "                                                  key_prefix='processing-jobs')\n",
    "print('Print S3 Path:', s3_path_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the training dataset to view the format we want to enforce through our processing job...\n",
    "\n",
    "To achieve this, we need to write a script that can perform the same transformations we executed directly from our notebook instance in Step 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have our label in the first column for XGBoost training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHALLENGE #1\n",
    "\n",
    "Using what you know about the data processing we previously performed to convert the raw data into the final training dataset, write the training pre-processing script that we will then utilize to run a SageMaker Processing job to convert our raw dataset into our training dataset.  \n",
    "\n",
    "*HINT: The following [SageMaker Notebook Example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/scikit_learn_data_processing_and_model_evaluation.ipynb) provides an example for running preprocessing scripts inside processing jobs using the scikit-image. Keep in mind the script is only an example. The one we create below  should mimic the transformations specific to our use case including: \n",
    "          -  Dropping non-impacting features\n",
    "          -  Converting categorical data to numeric \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile preprocessing.py\n",
    "\n",
    "# [Enter preprocessing code here - and convert the cell to code block ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the script we created above as a SageMaker Processing job...\n",
    "\n",
    "**CHALLENGE HELP:** If you are stuck on the preprocessing script above, comment out the line below and execute the cell to create your preprocessing.py script automatically by pulling in a pre-created version that exists in the /scripts folder of our notebook instance.   To uncomment simply remove the '#' and Run cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use an existing script instead of creating your own - remove the '#' from  the line below and execute\n",
    "#!cp ./scripts/preprocessing.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Processing Setup\n",
    "\n",
    "To run the scikit-learn preprocessing script as a processing job, create a SKLearnProcessor, which lets you run scripts inside of processing jobs using the scikit-learn image provided. Notice you get to select the instance type and size allowing you to perform processing/cleansing on a much larger instance size that is automatically provisioned and shutdown once processing completes - keeping your notebook size small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the preprocessing.py script created above as a processing job using the SKLearnProcessor.run() method. \n",
    "\n",
    "\n",
    "Input(s):\n",
    "\n",
    "   * Processing script (custom code)\n",
    "   * PCard raw dataset in Amazon S3 as *ProcessingInput* which is read by the script from /opt/ml/processing/input. \n",
    "   \n",
    "   \n",
    "Output(s): \n",
    "   * Source - path the script writes output data to.  For outputs, the destination defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format *s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name/*.\n",
    "    \n",
    "   * Output_name - values to make it easier to retrieve these output artifacts after the job is run\n",
    "    \n",
    "\n",
    "These local paths inside the processing container must begin with /opt/ml/processing/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=s3_path_to_data,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='validation_data',\n",
    "                                                source='/opt/ml/processing/validation')]\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'validation_data':\n",
    "        preprocessed_validation_data = output['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect the output of the pre-processing job, which consists of the processed features. The goal is to get the data in the same format as the training data above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.read_csv(preprocessed_training_data + '/trainp.csv', index_col=None)\n",
    "print('Training features shape: {}'.format(training_features.shape))\n",
    "training_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratuations - You've completed Lab 1 \n",
    "\n",
    "In this lab we explored our dataset, identified features for training and converted categorical features numeric.  We walked through the example of creating our training/validation dataset utilizing the notebook instance compute/memory as well as how you can utilize SageMaker Processing for data cleansing/pre-processing.\n",
    "\n",
    "We only needed one training dataset for our next lab so will only utilize one of the datasets created above.  The purpose of creating two was purely to demonstrate options for data cleansing/processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect & store variables we will need to use for Lab2\n",
    "# **NOTE: This step is only needed for the nature of our lab setup/environment\n",
    "\n",
    "%store test_data\n",
    "%store data_prefix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
