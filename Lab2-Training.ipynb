{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Training using Amazon SageMaker\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "   In this lab, we'll train a model using XGBoost and the training dataset created in Lab 1.  In a  real data science development lifecycle, this would be one of many experiments.\n",
    "   \n",
    "   * **Lab Outcome:**: The outcome of this lab is to create a trained model resulting in SageMaker model artifact that we will then deploy and evaluate in Lab3\n",
    "\n",
    "**Dependendencies:**\n",
    "   \n",
    "   1. This lab requires the training/validation/test datasets created in Lab1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 1: Configure Training Job\n",
    "\n",
    "Setup/Configure training job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "model_prefix = 'workshop/model'\n",
    "\n",
    "\n",
    "# customize to your bucket where you have stored the data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region, bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get XGBoost container image**\n",
    "\n",
    "We are utilizing SageMaker's XGBoost built-in-algorithm so we will pull the managed image for the appropriate region below. \n",
    "\n",
    "*You can Ignore the Warning indicating a newer version is available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data input paths from Lab 1\n",
    "Set variables pointing to our training/validation data created in Lab1. We are pulling in data_prefix which was a string variable set in Lab1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the stored variables (variables stored in Lab1)\n",
    "%store -r data_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, data_prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, data_prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Execute Training Job\n",
    "\n",
    "\n",
    "The example below illustrates kicking off training with the Amazon SageMaker Python SDK.  You can also kick off the training job using AWS SDK for Python - Boto3 using the *create_training_job* method. \n",
    "\n",
    "**References:**\n",
    "\n",
    "  * *[Common Parameters for Built-In Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html):* Reference specifying common parameters for Amazon SageMaker algorithms such as XGBoost\n",
    "  \n",
    "**Amazon SageMaker XGBoost Notes:**\n",
    "\n",
    "  * **[XGBoost Hyperparameters:](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)** \n",
    "      * We are going to experiment with an initial set of hyperparameters setting our objective to binary:logistic meaning  we are going to perform logistic regression for binary classification.   For this objective, the expected output is a probability. \n",
    "      * We have not explicitly identified an evaluation metric. By default, SageMaker will assign an evaluation metrics based on the objective set.  In this case, train:error and validation:error.  \n",
    "      * Imbalanced Dataset: From Lab1, we know we have an imbalanced dataset where the number of transactions identified as a recurring payment is much lower than non-recurring payments (class imbalance).  One experiment we will try below is to adjust the scale_pos_weight hyperparameter.  A general recommendation is: \n",
    "                  scale_pos_weight = sum(negative classes)/sum(positive classes) \n",
    "                  From Lab1: Negative Classes = 208715, Positive Classes = 2358\n",
    "                             208715/2358 = ~89\n",
    " \n",
    "      \n",
    "  * **Instance Type:** \n",
    "       * Amazon SageMaker XGBoost currently only trains using CPUs. It is a memory-bound (as opposed to compute-bound) algorithm. So, a general-purpose compute instance (for example, M4) is a better choice than a compute-optimized instance (for example, C4). Further, we recommend that you have enough total memory in selected instances to hold the training data. Although it supports the use of disk space to handle data that does not fit into main memory (the out-of-core feature available with the libsvm input mode), writing cache files onto disk slows the algorithm processing time.\n",
    "       * During experimentation and for small datasets, you can also choose to train on your [notebook locally](https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/) using the compute/memory of your notebook instance as opposed to training instances.  However, this is only recommended for small experience to ensure your notebook is not overprovisioned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "# Converting datetime object to string\n",
    "from datetime import datetime\n",
    "dateTimeObj = datetime.now() \n",
    "timestampStr = dateTimeObj.strftime(\"%d%m%Y-%H%M%S%f\")\n",
    "training_job_name = 'sagemaker-xgboost-workshop-' +  timestampStr\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, model_prefix),\n",
    "                                    enable_sagemaker_metrics=True,\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        scale_pos_weight=89,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation},job_name=training_job_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model Artifact in S3 \n",
    "\n",
    "Once the training job is complete, the model will be output to an S3 bucket.  This model artifact will be used for hosting our model and getting predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Markdown\n",
    "\n",
    "s3_model_artifact = 'https://s3.console.aws.amazon.com/s3/buckets/'+ bucket + '/' + model_prefix + '/output/' + training_job_name + '/output/?region=us-east-1&tab=overview'\n",
    "display(Markdown('S3 Model Artifact: [link]('+s3_model_artifact+')'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Cloudwatch Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sagemaker.analytics import TrainingJobAnalytics\n",
    "\n",
    "validation_metric_name = 'validation:error'\n",
    "validation_metrics_dataframe = TrainingJobAnalytics(training_job_name=training_job_name,metric_names=[validation_metric_name]).dataframe()\n",
    "validation_metrics_dataframe.head()\n",
    "\n",
    "training_metric_name = 'train:error'\n",
    "training_metrics_dataframe = TrainingJobAnalytics(training_job_name=training_job_name,metric_names=[training_metric_name]).dataframe()\n",
    "training_metrics_dataframe.head()\n",
    "\n",
    "metrics = training_metrics_dataframe.append(validation_metrics_dataframe)\n",
    "metrics.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Training Job in SageMaker Console \n",
    "\n",
    "You will see training logs as output in the notebook above; however, you can also view your training job from the SageMaker console as well.  You can also scroll down to monitor and evaluate system metrics (CPU/Memory/Disk) to help in future right sizing of training instances for cost optimization and performance.  Click [HERE](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/) to find your training job within the console and view metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratuations - You've completed Lab2\n",
    "\n",
    "In this lab we utilized the training/validation datasets created in Lab 1 to train a model.  In the next lab, we'll host the model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect & store variables we will need to use for Lab3\n",
    "\n",
    "%store training_job_name\n",
    "%store data_prefix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
