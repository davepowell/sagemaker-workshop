{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Hosting & Evaluation\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "   In this lab, we'll explore the options for hosting your model built in Lab2 on SageMaker as  well as evaluate how well our model is performing when trying to predict whether a transaction is recurring. \n",
    "   \n",
    "   * **Lab Outcome:** The outcome of this lab is to demonstrate multiple hosting options as well as evaluate our model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup & Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the stored variables (variables stored in Lab 2)\n",
    "%store -r training_job_name\n",
    "%store -r data_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer    # Converts strings for HTTP POST requests on inference\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'workshop/hosting'\n",
    "\n",
    "print('S3 Bucket for model artifact:', bucket)\n",
    "print('S3 Prefix for model artifact:', prefix)\n",
    "print('S3 Prefix for model evaluation data:', data_prefix)\n",
    "\n",
    "\n",
    "# customize to your bucket where you have stored the data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region, bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Model Hosting & Evaluation - Persistent Endpoints\n",
    "\n",
    "In this step, we'll explore hosting our model using a real-time persistent endpoint.  We are going to show both methods for hosting including: (1) Persistent Endpoints ~AND~ (2) Batch Transform.   Although we will showcase this purely to show the hosting options on Amazon SageMaker, the choice between utilizing persistent endpoints vs batch transform should really be decided based on the use case.  \n",
    "\n",
    "### Configure & Deploy Endpoint\n",
    "\n",
    "Below we will be hosting a single model (build in Lab2) behind a persistent endpoint and utilizing that endpoint for inference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model trained in Lab2 \n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# attach() is a method in the SageMaker SDK that attaches to an existing training job and creates an estimator\n",
    "# bound to that training job.  We are not re-training - we attaching to the training job.  If it is completed,\n",
    "# we can then deploy.  In this case we already know it completed in Lab 2 and are just attaching to deploy. \n",
    "xgb = Estimator.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've attached to the model object from Lab 2, let's setup our endpoint and deploy..\n",
    "\n",
    "*Note: You could optionally use AWS Python SDK to deploy (create_endpoint_config, create_endpoint).   In this example we are using the [SageMaker Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) which abstracts the underlying calls.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure endpoint & deploy\n",
    "\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "There are many ways to evaluate the performance of a machine learning model, but let's start by simply comparing actual to predicted values. In this case, we're trying to predicting whether a given transaction is a recurring payment (1) or not a recurring payment (0). \n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint. Our data is currently stored as NumPy arrays in memory of our notebook instance. To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV.\n",
    "\n",
    "Note: For inference with CSV format, SageMaker XGBoost requires that the data does NOT include the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer    # Converts strings for HTTP POST requests on inference\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "\n",
    "Loop over our test dataset\n",
    "Split it into mini-batches of rows\n",
    "Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r test_data\n",
    "predictions = predict(test_data.drop(['Recurring_Label_No', 'Recurring_Label_Yes'], axis=1).as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Confusion Matrix is one method for evaluating your model as it allows you to visualize the accuracy of model by comparing actual and predicted values.  In this case, we are evaluating whether a transaction is a recurring payment (1) or not a recurring payment (0).  Let's create a confusion matrix to evaluate our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_cm1 = pd.crosstab(index=test_data['Recurring_Label_Yes'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])\n",
    "tp = df_cm1.iloc[0,0]\n",
    "fp = df_cm1.iloc[0,1]\n",
    "fn = df_cm1.iloc[1,0]\n",
    "tn = df_cm1.iloc[1,1]\n",
    "results = [tp,fp,fn,tn]\n",
    "\n",
    "# display results\n",
    "sns.heatmap(df_cm1, annot=True, fmt='d', cmap=\"YlGnBu\").set_title('Confusion Matrix')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a Confusion Matrix: \n",
    "\n",
    "| Actuals/Predicted |  Prediction-RecurringNo     | Prediction-RecurringNo |\n",
    "| ----- | --------- | ------|\n",
    "| Actual-RecurringNo |  TRUE NEGATIVE (TN)     | FALSE POSITIVE (FP) |\n",
    "| Actual-RecurringYes | FALSE NEGATIVE (FN) | TRUE POSITIVE (TP)    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the values above to calculate some additional metrics for evaluation of the model.  There are many packages/libraries you can use to automatically calculate metrics; however, we are going to create a function to manually calculate metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(results):\n",
    "   #Manually calculate metrics\n",
    "\n",
    "   precision = (tp/(tp+fp))\n",
    "   print('Precision is: ', precision)\n",
    "   recall = (tp/(tp+fn))\n",
    "   print('Recall is: ', recall)\n",
    "   accuracy = ((tp + tn)/(tp+fp+fn+tn))\n",
    "   print('Accuracy is: ', accuracy)\n",
    "   f1 = (2*(precision*recall)/(precision+recall))\n",
    "   print('F1 Score is: ', f1)\n",
    "   fpr = (fp/(fp+tn))\n",
    "   print('False Positive rate is:', fpr)\n",
    "   tpr = (tp/(tp+fn))\n",
    "   print('True Positive rate is:', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important point here is that because of the *np.round()* function above we are using a simple threshold (or cutoff) of 0.5. Our predictions from xgboost with binary:logistic as the objective come out as continuous values between 0 and 1 and we force them into the binary classes that we began with. However, because identifying a transaction as recurring when it is not may result a negative customer experience by sending customer unnessary communication we may want to adjust the threshold. \n",
    "\n",
    "Let's first look at the continuous values of our predictions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous value predictions coming from our model tend to skew toward 0 (Not Recurring), which is expected given our use case.  However, there are some values in the 70-80 range that may be lower than acceptable to ensure we are not unnecessarily sending customer communication on recurring transactions.  Let's adjust the cutoff from .5 to .9 to lower our false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))\n",
    "\n",
    "df_cm2 = pd.crosstab(index=test_data['Recurring_Label_Yes'], columns=np.where(predictions > 0.9,1,0), rownames=['actuals'], colnames=['predictions'])\n",
    "tp = df_cm2.iloc[0,0]\n",
    "fp = df_cm2.iloc[0,1]\n",
    "fn = df_cm2.iloc[1,0]\n",
    "tn = df_cm2.iloc[1,1]\n",
    "results = [tp,fp,fn,tn]\n",
    "\n",
    "# display results\n",
    "sns.heatmap(df_cm2, annot=True, fmt='d', cmap=\"YlGnBu\").set_title('Confusion Matrix')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our false positives have been reduced, so let's take a second look at our metrics with the new boundary set to .8..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Endpoint Configuration & Endpoint in SageMaker Console \n",
    "\n",
    "You will see the endpoint created in the output above; however, you can also view your endpoint configuration as well as your endpoint from the SageMaker console as well.  \n",
    "\n",
    "* Click [HERE](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig/) to view your endpoint configuration from the console. \n",
    "* Click [HERE](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig/) to view your endpoint from the console. You can also scroll down to monitor and evaluate system metrics (CPU/Memory/Disk) to help in future right sizing of hosting instances for cost optimization and performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Hosting & Evaluation - Batch Transform\n",
    "\n",
    "In the previous step we created a persistent endpoint that we could use to make real-time predictions.  With real-time endpoints, they stay up and running until you shut them down.  You can view the endpoint created above in the SageMaker service console under [**Inference --> Endpoints**](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints).  There are many use cases, such as forecasting, where you do not need a persistent endpoint because you are submitting batch prediction and obtaining batch results on an ad-hoc or scheduled basis. In these cases, [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) is a cost effective option. \n",
    "\n",
    "### Batch Transform Setup\n",
    "In this step, we'll explore hosting our model using batch transform. With batch transform, we will send in batch predictions and receive batch results.  For demonstration, we will utilize the same data formatting performed above for test_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch transform expects prediction input to be in S3. \n",
    "# So we need to load the same data used in our inference above to S3 as a single batch prediction *.csv.\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "df = pd.DataFrame(test_data)\n",
    "\n",
    "#Drop labels\n",
    "df.drop(labels=['Recurring_Label_No', 'Recurring_Label_Yes'], axis=1, inplace=True)\n",
    "df.to_csv('./test.csv',header=False, index=False, )\n",
    "#predictions = predict(test_data.drop(['Recurring_Label_No', 'Recurring_Label_Yes'], axis=1).as_matrix())\n",
    "\n",
    "#Upload to S3\n",
    "data_prefix = 'workshop/data'\n",
    "s3_test_data = boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(data_prefix, 'test/test.csv')).upload_file('./test.csv')\n",
    "\n",
    "# S3 bucket for saving batch prediction results\n",
    "batch_output_prefix = 'workshop/batch-predictions-out'\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting datetime object to string\n",
    "from datetime import datetime\n",
    "dateTimeObj = datetime.now() \n",
    "timestampStr = dateTimeObj.strftime(\"%d%m%Y-%H%M%S%f\")\n",
    "job_name = 'sagemaker-xgboost-workshop-' +  timestampStr\n",
    "model_name = training_job_name\n",
    "\n",
    "output_data_path = 's3://{}/{}'.format(bucket, batch_output_prefix)\n",
    "batch_prediction_data  = 's3://{}/{}/test/test.csv'.format(bucket, data_prefix)\n",
    "job_name = 'serial-inference-batch-' + timestampStr\n",
    "\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='serial-inference-batch',\n",
    "    sagemaker_session=sess,\n",
    "    accept = CONTENT_TYPE_CSV\n",
    ")\n",
    "transformer.transform(data = batch_prediction_data,\n",
    "                      job_name = job_name,\n",
    "                      content_type = CONTENT_TYPE_CSV,\n",
    "                      join_source='Input',\n",
    "                      split_type = 'Line')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Output Results\n",
    "\n",
    "After the transform job above is done, download the output from the S3 location we specified on *output_path*. Because we specified *join_source='Input'*  above, our output will include both the data sent for inference as well as the prediction result.  Without that parameter, only the prediction result is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local filesystem\n",
    "batch_output = transformer.output_path\n",
    "!mkdir -p batch_data/output\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output/\n",
    "\n",
    "# Head to see what the batch output looks like\n",
    "!head batch_data/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations - You've completed Lab 3\n",
    "\n",
    "In this lab we explored hosting our model using both a real-time endpoint as well as using batch transform.  We also evaluated the model for success against the ML problem we are trying to solve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
